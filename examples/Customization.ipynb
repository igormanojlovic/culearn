{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cumulant Learning with Transformers and Custom Datasets\n",
    "\n",
    "This notebook shows how to apply Cumulant Learning with a custom transformer-based regression model and a custom pandas dataset. The transformers and pandas are chosen for this example due to their popularity, but the learning process is open for other customizations as well."
   ],
   "id": "244028747ca4be31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Custom Regression Model\n",
    "\n",
    "We will be using [`PositionEmbedding`](https://keras.io/keras_hub/api/modeling_layers/position_embedding/), [`TransformerEncoder`](https://keras.io/keras_hub/api/modeling_layers/transformer_encoder/), and [`TransformerDecoder`](https://keras.io/keras_hub/api/modeling_layers/transformer_decoder/) from [Keras-NLP](https://pypi.org/project/keras-nlp/) to build our custom Sequence-to-Sequence (S2S) regression model `S2S_Transformers` that inherits the `culearn.regression.S2S_DNN` class."
   ],
   "id": "8611dcf36fcce30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install keras-nlp",
   "id": "866b861cc1a65b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from culearn.regression import *\n",
    "from keras_nlp.layers import TransformerEncoder, TransformerDecoder, PositionEmbedding\n",
    "\n",
    "\n",
    "class S2S_Transformers(S2S_DNN):\n",
    "    def __init__(self,\n",
    "                 depth: 'int > 0' = 2,\n",
    "                 n_heads: 'int > 0' = 4,\n",
    "                 embedding_dim: 'int > 0' = 64,\n",
    "                 hidden_dim: 'int > 0' = 128,\n",
    "                 max_length: 'int > 0' = 1000,\n",
    "                 drop: 'float >= 0' = 0.1,\n",
    "                 *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Sequence-to-Sequence (S2S) model with Transformer encoder and decoder.\n",
    "\n",
    "        :param depth: Number of encoder-decoder layers.\n",
    "        :param n_heads: Number of Transformer heads.\n",
    "        :param embedding_dim: Number of feedforward units in input embedding layer.\n",
    "        :param hidden_dim: Number of feedforward units within Transformer layers.\n",
    "        :param max_length: Maximum number of positions in input sequences.\n",
    "        :param drop: Dropout rate for Transformer layers.\n",
    "        :param args: Base class arguments.\n",
    "        :param kwargs: Base class key-value arguments.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.x_projection = tfl.Dense(embedding_dim)\n",
    "        self.y_projection = tfl.Dense(embedding_dim)\n",
    "        self.positioning = PositionEmbedding(max_length)\n",
    "        self.encoders = [\n",
    "            TransformerEncoder(\n",
    "                num_heads=n_heads,\n",
    "                intermediate_dim=hidden_dim,\n",
    "                dropout=drop)\n",
    "            for i in range(depth)\n",
    "        ]\n",
    "        self.decoders = [\n",
    "            TransformerDecoder(\n",
    "                num_heads=n_heads,\n",
    "                intermediate_dim=hidden_dim,\n",
    "                dropout=drop)\n",
    "            for i in range(depth)\n",
    "        ]\n",
    "\n",
    "    def _s2s(self, y_past, x_future):\n",
    "        # Encode past sequence.\n",
    "        y_encoded = self.y_projection(y_past)\n",
    "        y_encoded = self.positioning(y_encoded)\n",
    "        encoder_outputs = []\n",
    "        for encoder in self.encoders:\n",
    "            y_encoded = encoder(y_encoded)\n",
    "            encoder_outputs.append(y_encoded)\n",
    "\n",
    "        # Decode future sequence using corresponding encoder output.\n",
    "        x_encoded = self.x_projection(x_future)\n",
    "        x_encoded = self.positioning(x_encoded)\n",
    "        decoder_output = x_encoded\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            decoder_output = decoder(decoder_output, encoder_outputs[i])\n",
    "\n",
    "        # Return the output from the last decoder layer directly.\n",
    "        return decoder_output"
   ],
   "id": "1d3fe161b1852ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Custom Dataset\n",
    "\n",
    "We will generate random numbers to represent 10 X and 100 Y time series at one-hour resolution in one-year time interval. For this example, we will store the values in memory using one `pandas.DataFrame` for X and one `TimeSeriesInMemory` wrapper around multiple `pandas.Series` for Y (to support parallel processing). If you don't have enough memory to do that for your own dataset and you need some kind of lazy loading, you can implement your own `TimeSeries` subclass analog to `culearn.base.TimeSeriesInMemory` and `culearn.csv.TimeSeriesCSV`. You can also take a look at different implementations of the `DataSource` class in the `culearn.data` module to see how these classes are used on some real datasets."
   ],
   "id": "86c4d97a9bcea215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from culearn.base import *\n",
    "\n",
    "resolution = TimeResolution(hours=1)\n",
    "interval = TimeInterval(datetime(2021, 1, 1), datetime(2022, 1, 1))\n",
    "timestamps = pd.DatetimeIndex([_.start for _ in resolution.steps(interval)])\n",
    "dataset = PredictionDataset(\n",
    "    x = pd.DataFrame(np.random.rand(len(timestamps), 10), index=timestamps),\n",
    "    y = [\n",
    "        TimeSeriesInMemory(\n",
    "            TimeSeriesID(str(i)),\n",
    "            pd.Series(np.random.rand(len(timestamps)), index=timestamps))\n",
    "        for i in range(100)\n",
    "    ]\n",
    ")"
   ],
   "id": "2251cc204715f33d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cumulant Learning\n",
    "\n",
    "Once we have the regression model and dataset ready, we can apply Cumulant Learning."
   ],
   "id": "511384d23df1a2d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from culearn.learn import *\n",
    "\n",
    "# Prepare the regressors\n",
    "one_day = timedelta(1)\n",
    "horizon = int(one_day / resolution)\n",
    "regressor = lambda: TimeSeriesRegressor(horizon, base=DeepS2S(epochs=1, hidden=lambda:S2S_Transformers()))\n",
    "\n",
    "# Prepare the learner\n",
    "learner = CumulantLearner(dataset, resolution, CumulantTransform(), regressor)\n",
    "\n",
    "# Train the learner\n",
    "fit_interval = TimeInterval(interval.start, interval.end - one_day)\n",
    "learner.fit(fit_interval, verbose=True)\n",
    "\n",
    "# Test the learner\n",
    "pred_intervals = learner.predict(fit_interval.end, p=[0.75, 0.95, 0.99], clusters=True, members=True)\n",
    "for pi in pred_intervals:\n",
    "  print(pi.ts_id)\n",
    "  display(pi.to_frame())\n",
    "  break\n",
    "pred_cumulants = learner.predict_cumulants(fit_interval.end)\n",
    "for pc in pred_cumulants:\n",
    "  print(pc.ts_id)\n",
    "  display(pc)\n",
    "  break\n",
    "pred_figure = learner.figure(fit_interval.end, p=[0.75, 0.95, 0.99])\n",
    "pred_figure.show()"
   ],
   "id": "2afaab5647dce980",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
